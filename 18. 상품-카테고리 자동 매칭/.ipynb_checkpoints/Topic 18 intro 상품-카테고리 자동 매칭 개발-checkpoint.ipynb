{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 상품-카테고리 자동 매칭 공부\n",
    "\n",
    "Naver Campus Hackday에서 주제중에 상품-카테고리 자동 매칭 개발 주제가 있다. ML 의 기본같은 주제기도 하고, 내가 지원한 주제여서 정리를 해보려고 한다! \n",
    "\n",
    "--- \n",
    "\n",
    "#### 일단 대회 깃헙에 올라온 Problem Identification!\n",
    "- 주제?\n",
    "\n",
    "업무 효율화를 위한 AI를 기반 상품 자동 카테고리 매칭 시스템\n",
    "    - 보통 'Product Categorization' Problem 으로 불림\n",
    "    \n",
    "Other Requirements: \n",
    " - tensorflow, keras 활용 (Deep Learning 사용)\n",
    " - Input Data? 상품명, 브랜드, 제조사 등 텍스트로 구성된 상품데이터\n",
    " - Output Result? 해당 상품의 카테고리\n",
    " - 미리 만들어진 정답셋이 있음 == We have training set, validation set, testing set\n",
    " - 목표: Test set 기준 Accuracy 80%\n",
    "    - Alpha: 모델 설계시 문서화\n",
    "    - 모델 검증 코드/프로세스 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보통 상품 - 카테고리 프로젝트의 흐름\n",
    "\n",
    "###  어떤 종류의 데이터가 있는지 알아보기\n",
    "\n",
    "- 제품 사진이 있는 경우: 이미지 -> CNN 사용 (pretrained model - transfer learning 사용하기)\n",
    "\n",
    "- 제품 이름: 몇 단어로 이루어진 정보  -> 텍스트를 숫자로 바꿔야 함 Tokenization/Embedding\n",
    "- 상품 번호 : 숫자 -> 간단함\n",
    "\n",
    "- 리뷰 (?): 문장으로 이루어짐 -> ~의, ~는/은/이/가 등의 조사를 지우고 embedding 을 해야함 \n",
    "\n",
    "--- \n",
    "숫자로 이루어진 데이터는 preprocessing 이 많이 필요하진 않지만, 텍스트를 컴퓨터가 알아들을 수 있도록 바꿔야 한다. 어떻게? \n",
    "\n",
    "\n",
    "- Bag-of-Words: unique 한 모든 단어를 n-차원의 벡터로 바꾼다. 단어가 몇번 나오는지 word frequency 를 사용.\n",
    "     - 장점: 쉬움\n",
    "     - 단점: syntax 무시. 메모리 많이 사용함 (0이 엄청 많은 n 차원의 벡터로 만든다)\n",
    "\n",
    "\n",
    "- TF-IDF (Frequency-inverse Document Frequency): bag-of-words와 비슷. But! occurence 가 적을수록 higher -> 더 자세한 설명일 가능성이 높기 때문에. overall frequency 를 구해서 적게 나온건 제외하고 n 차원의 벡터로 만든다\n",
    "    - 장점: bag-of-words 보다 dimension 이 작아서 메모리 덜 씀\n",
    "\n",
    "\n",
    "- Word2Vec: Sparse 하게 맵핑하는걸 방지함. 단어의 context를 예측하는 2 layer neural 를 만든다. \n",
    "    - 장점: sparse data 줄어들어서 메모리 많이 아낌. 단어들 사이의 semantic/syntax를 잘 잡아낼 수 있음 \n",
    "    - 단점: complex computation \n",
    "\n",
    "\n",
    "\n",
    "내용으로만 보면 word2vec이 제일 좋아보이지만, 결과를 비교해보면 tf-idf 가 좋을 때가 많음 - simplier the better (Occam's Razar)\n",
    "\n",
    "\n",
    "Rich Syntax가 필요한 작업일 때는 word2vec이 좋겠지만... 그냥 카테고리 정도를 예측하는 단순한 작업에서는 이정도의 computation 을 사용해서 할 필요는 없음... 맵핑을 해서 딴 작업에도 쓸꺼면 (e.g. recommendation) word2vec이 효율적이겠지만, 분류로는 오버하는 느낌? \n",
    "\n",
    "---\n",
    "\n",
    "### 어떤 알고리즘 쓸건지 정하기 \n",
    "\n",
    "- 이미지: CNN 쓰기 - transfer learning (inception-resnet?) Xception? 가벼운건 MobileNet, Inception v3\n",
    "\n",
    "- 텍스트: Scikit-learn에 있는... Naive Bayes, Logistic Regression, kNN, Random Forest, SVM, Gradient Boosting \n",
    "\n",
    "내가 읽은 포스트에서는 tf-idf를 사용해서 vectorize한 다음 logistic regression 으로 분류한게 제일 성능 좋았음... 제일 간단한거 =? 제일 좋은거?\n",
    "\n",
    "---\n",
    "\n",
    "### 다양한 알고리즘의 결과 합치기\n",
    "- 단순한 방법: class probability 의 평균/합을 구하기\n",
    "- Ensemble: 다른 모델의 아웃풋을 받아서 사용하는 알고리즘\n",
    "\n",
    "---\n",
    "\n",
    "### 결과 내기\n",
    "- 1개의 카테고리로 분류할 것인지, hierarchy 가 있는 정보로 분류 할것인지, 태그 같이 다양한 카테고리로 분류할 것인지...?\n",
    "- 1개의 카테고리면 class probability 가 가장 높은거 내면 될테고\n",
    "- 1개 이상의 카테고리로 분류하는거면 threshold probability 를 정해야 하는데... 예를 들면 확률이 0.6 이상이면 그 카테고리에 든다! 라고 할것인지, 그 값을 0.7, 0.8로 높일 것인지..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시를 보면서 배우기~ \n",
    "\n",
    "### 1. Kaggle Example Competition 데이터셋 \n",
    "\n",
    "연습해 볼 수 있는 데이터셋: https://www.kaggle.com/c/otto-group-product-classification-challenge\\n \n",
    "\n",
    "- 데이터에 대한 정보: \n",
    "    - Row 마다 다른 product 임. 총 93개의 feature 가 있음. Feature 의 이름은 없고, 그냥 1, 2, 3으로 불림.  \n",
    "    - 카테고리는 9개 있음 \n",
    "    - Training set (144k 개) 와 Test set(62k 개)로 나뉨 --> 6MB \n",
    "    - feature들이 다 숫자로 이루어져 있어서 preprocessing 많이 안해도 되는 장점?단점?이... \n",
    "\n",
    "\n",
    "- 접근 모델?\n",
    "    1. XGBoost:  gradient boosting algorithm. 수백?수천? 개의 Decision Tree 로 이루어졌음 \n",
    "    2. LogisticRegression\n",
    "    3. RandomForestClassifier\n",
    "        +) 마지막에 Ensemble of different types of classifiers\n",
    "\n",
    "---\n",
    "\n",
    "대학생들이 학교 프로젝트로 만든 kaggle competition 풀이인데, 이것저것 간단한 방법들 시도해서 성능 비교했음. 마지막에 앙상블도 했음. overview 라고 하기에는 너무 길어서 좀 풀어쓴거 읽고싶으면 보면 될듯 : http://www.cse.scu.edu/~mwang2/projects/ML_KaggleCompetition_15s.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Classifier 예시 - 블로그 \n",
    "\n",
    "- Product 이름으로 classification 했음 \n",
    "    - text 로 이루어졌기 때문에 Preprocessing이 중요함 \n",
    "    \n",
    "    \n",
    "- 영어로 된 product name이라서...  Preprocessing 한 방법: \n",
    "    - A. Lowercasing all letters. \n",
    "    - B. Removing punctuation and special characters (like *, | or .). We keep hyphens to preserve information in cases like “t-shirts” \n",
    "    - C. Removing stopwords (the, and, in, etc.) because we do not expect them to have much predictive value.\n",
    "    - D. Lemmatizing words (≈finding word stems) to remove variance from word inflection (i.e. we want our model to know that “apples” and “apple” refer to the same thing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Product Categorization using ML 책 정리 \n",
    "Author: Nicklas Lindgren - 다양한 알고리즘 종류! \n",
    "\n",
    "A. Naive Bayes Algorithm \n",
    "    * Classification 알고리즘임  \n",
    "    * P(b|a) = P(a|b)P(b) / P(a) \n",
    "    * 가정: 모든 feature -> independently to the outcome of the classificaiton\n",
    "    * 'Naive' 인 이유는: 가정 자체가 independent 라고 정의해서, 실제는 안그럴 가능성이 커서 \n",
    "\n",
    "\n",
    "\n",
    "B. Support Vector Machine (SVM)  \n",
    "    * Learns the mapping x -> y. x: object, y: label (target) \n",
    "    * Assigns every entry to a point in a vector space: n 차원의 벡터로 제이터를 표기하는 방법. \n",
    "    * Separation line 이 있어서 분류함 \n",
    "\n",
    "\n",
    "C. DBSCAN (Density based spatial clustering of applications with noise    \n",
    "    * k-mean와 비슷한 방법인데, k-mean과 달리 k 를 알필요가 없음\n",
    "\n",
    "\n",
    "\n",
    "얻어갈 것? \n",
    "    - 데이터를 normalize 했음 (rescaling): 결과 훨씬 좋아짐 \n",
    "    - shuffling\n",
    "    - SVM 의 성능이 훨씬 좋음\n",
    "    \n",
    "- Text 분류 논문: https://www.csie.ntu.edu.tw/~cjlin/papers/libshorttext.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 텍스트 분류 - 긍정? 부정? \n",
    "\n",
    "출처: TF 튜토리얼https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
    "\n",
    "* 사람들이 쓴 영화 리뷰로부터 긍정적인지 부정적인지 레이블링 \n",
    "* 글 --> Array of integers --> Vectors  \n",
    "* One-Hot-Encode: 메모리를 너무 많이 쓰게 됨  \n",
    "* Embedding Layer : max_length \n",
    "* num_reviews: pad arrays so they all have the same length, create an integer tensor \n",
    "\n",
    "\n",
    "Architecture:\n",
    "    1. Embedding Layer: 결과: (batch, sequence, embedding)  \n",
    "    \n",
    "    2. Pooling 1D layer: 결과: fixed-length output vector \n",
    "        - 왜? allow the model to handle input of variable length, in the simplest way possible \n",
    "        \n",
    "    3. Fully Connected Layer \n",
    "    \n",
    "    4. Sigmoid (0 or 1 니까)  - ReLU for categorical \n",
    "    \n",
    "        - simple example of cnn: https://www.tensorflow.org/tutorials/estimators/cnn \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 다른 간단한 예시\n",
    "\n",
    "\n",
    "블로그: https://medium.com/dataweave/implementing-a-machine-learning-based-ecommerce-product-classification-system-f846d894148b \n",
    "\n",
    "\n",
    "정리:\n",
    "    여기서는 어떻게 했나?  \n",
    "    - Multiclass SVM (97.5%) \n",
    "    - Random Forest (93.5%) \n",
    "    - Naive Bayes (91.1%) \n",
    "    - Decision Trees (90.2%) \n",
    "\n",
    "\n",
    "데이터: 영어로 된 제품의 이름 \n",
    "    - 소문자로 바꿈  \n",
    "    - special character (?!@# 등의 문자)를 없앰\n",
    "    - Stopping words (and, by, for같은 단어)를 없앰 -> 의미있는 것만 뽑으려고 한듯 \n",
    "    \"- Unigram & Biggram token 을 만듬  \n",
    "\n",
    "\"제품의 이름을 Bag of Words 모델을 사용해서 벡터로 변환함 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 예시 \n",
    "\n",
    "블로그: https://towardsdatascience.com/classifying-marketplace-inventory-at-scale-with-machine-learning-99e69eac585e\n",
    "\n",
    "정리:\n",
    "\n",
    "    - 여기서는 각 상품에 대해서 알맞은 태그를 만드는 것을 함 \n",
    "    - Logic Regression 이 성능이 좋았음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다른 실제 예시 - tensorflow: Text classification: 천천히 읽어보기~\n",
    "\n",
    "간단한 예시: END TO END 라서 preprocessing 할 때 참고 할 수 잇음\n",
    "\n",
    "https://github.com/quantitative-technologies/tensorflow-text-classification\n",
    "\n",
    "---\n",
    "\n",
    "설명이랑 같이 있는거 :\n",
    "https://towardsdatascience.com/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575\n",
    "\n",
    "---\n",
    "\n",
    "다양한 Text classification 예시들 링크 걸어줌: 좋은거 꽤 있는듯/ 모델 만들때 참고용\n",
    "\n",
    "https://github.com/fendouai/Awesome-Text-Classification\n",
    "\n",
    "---\n",
    "\n",
    "LSTM 사용한 예시! 엄청 자세하게 설명 해줌...\n",
    "    \"http://ruder.io/text-classification-tensorflow-estimators/\n",
    "\n",
    "---\n",
    "\n",
    "대학생들이 과제로 한 프로젝트인데 엄청 자세하게 다양한 모델 비교함. Kaggle 에서 나온 데이터 가지고 실험함. 앙상블 사용했으니까 그거 참고용 \n",
    "   http://www.cse.scu.edu/~mwang2/projects/ML_KaggleCompetition_15s.pdf\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "* 그 외에 간단한 예시 * \n",
    "아마존 상품 분류: 데이터로 이미지, 상품 이름, 상품 번호, 브랜드 있었음\n",
    "\n",
    "https://github.com/Sidhant-Chatterjee/Amazon-Product-Classification\n",
    "\n",
    "---\n",
    "\n",
    "텐플 공식 튜토리얼: 간단한 CNN 으로: syntax 참고용 \n",
    "- https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub\n",
    "\n",
    "- https://medium.com/tensorflow/building-a-text-classification-model-with-tensorflow-hub-and-estimators-3169e7aa568\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** 공부를 하면서... **\n",
    "\n",
    "보통 제품의 이름을 bag of words 를 사용해서 벡터로 만든다\n",
    "\n",
    "알고리즘이 생각하기에 비슷한 이름의 제품을 벡터화시켜서 embedding 해놓는 것이기 때문에 여기서 배운 embedding 이랑 mapping 을 상품 카테고리를 나누는데 뿐만 아니라 좀 더 발전 시켜서 recommendation system 에 쓸 수 있을 듯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
